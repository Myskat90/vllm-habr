# Часть 3: Написание скрипта vLLM для шардирования, распределения вычислений и Ray Serve для вывода API

В этой части мы рассмотрим, как организовать распределённый инференс при помощи **vLLM** и обеспечить доступ к нему через **Ray Serve**.  
Ниже описаны ключевые концепции шардирования, параллелизации вычислений и принципы работы API.

---

## 1. Что такое vLLM и почему он выбран

**vLLM** — это фреймворк, который поддерживает параллельный инференс больших языковых моделей, позволяя:

- **Шардировать модель** между несколькими GPU (tensor parallel).
- **Использовать NCCL** (NVIDIA Collective Communications Library) для синхронизации вычислений и передачи данных.
- **Отдавать ответы потоковым способом** (streaming), что особенно актуально для сценариев, аналогичных OpenAI ChatCompletion API.

Благодаря шардированию, мы можем запускать крупные модели, не ограничиваясь одной видеокартой.

---

## 2. Распределение инференса и параметры шардирования

**Распределённый инференс** происходит следующим образом:

1. **Tensor parallel size** указывает, на сколько частей делятся параметры модели и как они распределяются по GPU.
2. **Pipeline parallel size** при необходимости вводит конвейерную параллельность (pipeline), если модель можно разбить на несколько этапов обработки.
3. **gpu_memory_utilization** и **cpu_offload_gb** — параметры, помогающие эффективно расходовать видеопамять и часть объёма оперативной памяти хоста.

Поскольку vLLM использует NCCL, обмен данными между GPU идёт с высокой пропускной способностью, а модель может масштабироваться до больших размеров.

---

## 3. Что такое Ray Serve

**Ray Serve** — это компонент фреймворка Ray, предоставляющий микросервисную архитектуру для инференса:

- Позволяет развернуть **FastAPI**-приложение и автоматически управлять его репликами.
- Делает возможной горизонтальную масштабируемость (каждая реплика может быть на отдельном узле, если нужно).
- Может эффективно работать с GPU, когда под каждый сервис зарезервирована часть видеокарт.

Мы используем Ray Serve, чтобы предоставить внешний **HTTP API**, совместимый с OpenAI-протоколами (ChatCompletion), через который клиенты смогут делать запросы к модели, развёрнутой в vLLM.

---

## 4. Описания скриптов

### 4.1 Скрипт [serve.py](ray-serve-vllm/serve.py)
В этом скрипте:

- Инициируется **vLLM** с заданными параметрами (tensor parallel, pipeline parallel и т.д.).
- Создаётся FastAPI-приложение с эндпоинтами для **ChatCompletion** (стримингового и обычного).
- Добавляется **JWT-аутентификация** для контроля доступа.
- **Ray Serve** оборачивает это приложение и распределяет HTTP-запросы по рабочим процессам.

### 4.2 Скрипт [auth.py](ray-serve-vllm/auth.py)
Этот скрипт:

- Работает с **JWT** (создание и проверка токенов, роль пользователя).
- Загрузку пользователя (логин, роль, хэш пароля) берёт из переменных окружения (например, `USER_LIST`).
- Предоставляет вспомогательные функции для проверки роли (admin/user) и времени жизни токена.


> *Данный модуль подключается внутри `serve.py`, чтобы проверять токены при обращении к эндпоинтам.*

---

## 5. Итог: два скрипта для дальнейшего использования

- **serve.py**: основной сервер, где Ray Serve запускает FastAPI-приложение с vLLM под капотом.
- **auth.py**: вспомогательная логика JWT-аутентификации, используемая в серверном скрипте.

Благодаря этим двум модулям мы получаем распределённый инференс, шардирование больших языковых моделей и удобный API для взаимодействия с ними.  
В следующих частях статьи мы разберёмся, как собрать данные скрипты в Docker-образ и развернуть всё в KubeRay cluster.
